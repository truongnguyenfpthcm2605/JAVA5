<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">Podman Desktop: A Beginner’s Guide to Containerization</title><link rel="alternate" href="https://www.mastertheboss.com/soa-cloud/docker/podman-desktop-a-beginners-guide-to-containerization/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/soa-cloud/docker/podman-desktop-a-beginners-guide-to-containerization/</id><updated>2023-05-12T08:47:50Z</updated><content type="html">Podman is a popular containerization tool that allows users to manage containers, images, and other related resources. The Podman Desktop Tool is an easy-to-use graphical interface for managing Podman containers on your desktop. In this tutorial, we’ll go over how to use the Podman Desktop Tool to manage WildFly container image, covering some of its ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">Eclipse Vert.x 4.4.2 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-4-2" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-4-2</id><updated>2023-05-12T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.4.2 has just been released. It fixes CVE-2023-32081 and quite a few bugs that have been reported by the community</content><dc:creator>Julien Viet</dc:creator></entry><entry><title type="html">Simplifying Drools Tests with AssertJ</title><link rel="alternate" href="https://blog.kie.org/2023/05/simplifying-drools-tests-with-assertj.html" /><author><name>Paolo Bizzarri</name></author><id>https://blog.kie.org/2023/05/simplifying-drools-tests-with-assertj.html</id><updated>2023-05-11T12:10:59Z</updated><content type="html">INTRODUCTION Creating clean, concise, and maintainable tests is crucial for any software project. In this post, we will discuss the ongoing refactoring process for Drools tests and illustrate how these techniques can benefit other projects. By leveraging the expressiveness of AssertJ assertions, we aim to enhance readability and conciseness, leading to more efficient and easily comprehensible tests. THE ORIGINAL CODE The code used in this example comes from the drools project. The test class is BackwardChainingTest and the test method is testQueryWithOr. The code can be retrieved from here: In the original tests the block we are considering uses roughly 55 lines of code. Let’s start by examining a snippet from the original code: List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); Query Results results = ksession.getQueryResults("p", new Integer[]{2}); for (final QueryResultsRow result : results) { list.add((Integer) result.get("x")); } assertThat(list.size()).isEqualTo(1); assertThat(list.get(0).intValue()).isEqualTo(2); This code retrieves query results from a Drools knowledge session (ksession) and iterates through the results to add them to a list. It then asserts the list size and content.  While this code works, it’s not as concise or expressive as it could be. Let’s see how we can improve this code. STEP 1 – USE ASSERTJ ASSERTIONS The snippet presented before uses AssertJ assertions. We can improve the code by leveraging the assertions for collections of AssertJ. List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); QueryResults results = ksession.getQueryResults("p", new Integer[]{2}); for (final QueryResultsRow result : results) { list.add((Integer) result.get("x")); } assertThat(list).hasSize(1); assertThat(list).contains(2); These assertions are now clearer and simpler, but they can be simplified more. The first assertion states that list contains only one element, and the second assertion states that list contains exactly the element 2. So we can rewrite this snippet as follow: List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); QueryResults results = ksession.getQueryResults("p", new Integer[]{2}); for (final QueryResultsRow result : results) { list.add((Integer) result.get("x")); } assertThat(list).containsExactly(2); STEP 2 – CREATE COLLECTIONS ON THE FLY USING EXTRACTING This is already simpler and better, however there is still the rather ugly code represented by the creation of the list collection. We are creating the list collection just for the assertion. Fortunately AssertJ has a very powerful mechanism to create collections on the fly and use them for assertions, named extracting. With extracting it is possible to take a collection, extract one or more properties from all the elements of the collections and then use the resulting collection to perform assertions. Here is the code using extracting: QueryResults results = ksession.getQueryResults("p", new Integer[]{2}); assertThat(results).extracting(r -&gt; r.get("x")).containsExactly(2); We have now removed the code for initializing the variable list, and we are down only two lines. STEP 3 – INLINE VARIABLE IN A SINGLE ASSERTION Looking at the getQueryResult method of the session, it is clear that it already takes a vararg argument, so it is not necessary to use an array. QueryResults results = ksession.getQueryResults("p", 2); assertThat(results).extracting(r -&gt; r.get("x")).containsExactly(2); As an optional step, we can remove the array and inline the result variable, obtaining a much more compacted version of our original assertion: assertThat(ksession.getQueryResults("p",2)).extracting(r -&gt; r.get("x")).containsExactly(2); This is marked optional since some people prefer to keep intermediate variables on separate lines for debugging purposes. STEP 4 – THE FINAL RESULT Repeating the process for all the original 55 lines of codes in the test produces the final result: assertThat(ksession.getQueryResults("p", 0)).extracting(r -&gt; r.get("x")).isEmpty(); assertThat(ksession.getQueryResults("p", 1)).extracting(r -&gt; r.get("x")).containsExactly(1); assertThat(ksession.getQueryResults("p", 2)).extracting(r -&gt; r.get("x")).containsExactly(2); assertThat(ksession.getQueryResults("p", 3)).extracting(r -&gt; r.get("x")).containsExactly(3); assertThat(ksession.getQueryResults("p", 4)).extracting(r -&gt; r.get("x")).isEmpty(); assertThat(ksession.getQueryResults("p", 5)).extracting(r -&gt; r.get("x")).isEmpty(); assertThat(ksession.getQueryResults("p", 6)).extracting(r -&gt; r.get("x")).containsExactly(6, 6); This uses only 7 lines of codes and it is much more clear than the original version. CONCLUSIONS In our view the new version of the code is better for several reasons: * Conciseness: The code is simplified, making it easier to understand at a glance. * Readability: The new version is more expressive, making it clear what the test is asserting. * AssertJ Capabilities: By leveraging the AssertJ library, you can take advantage of its powerful assertions and extraction methods, leading to more maintainable and flexible tests. We hope you can reuse part or all of these patterns to improve the tests of other projects.  The post appeared first on .</content><dc:creator>Paolo Bizzarri</dc:creator></entry><entry><title type="html">WildFly Release Plans</title><link rel="alternate" href="https://wildfly.org//news/2023/05/11/WildFly-Roadmap/" /><author><name>Brian Stansberry</name></author><id>https://wildfly.org//news/2023/05/11/WildFly-Roadmap/</id><updated>2023-05-11T00:00:00Z</updated><content type="html">As the WildFly project continues to move on from the major effort involved with integrating Jakarta EE 10, it’s time to go back to the roughly time-boxed development and release model that worked so well from WildFly 12 until WildFly 26. During that period, the WildFly project followed a roughly time-boxed development model. Roughly every three months we produced a new WildFly major release, with a large set of features, enhancements and bug fixes. We didn’t operate on a strict time schedule, but we tried to avoid significant schedule delays just to bring in particular feature or set of features. If a feature didn’t make a particular release it could just go in the next one a few months later. At the beginning of 2022, I and toward a feature-boxed approach. We did this because it was a more effective way to implement the transition to EE 10. But with that work behind us, the consensus among the WildFly developers is that moving back to the old time-boxed apprach will allow more effective delivery of new features and fixes to our users. Our aim is to produce feature releases in January, April, July and October. Experience has shown us that those months work best for not having development work overly disrupt summer or year end holidays. We’ll continue with our practice of releasing a Beta two weeks before the Final release, except for the January release, where the beta will be out somewhat earlier to avoid doing any release work in the latter part of December. 2023 - 2024 RELEASE SCHEDULE The following is a rough schedule of when we intend to do feature releases over the remainder of 2023 and through 2024. Please note that these dates are subject to change. Version Beta Release Final Release WildFly 29 June 29, 2023 July 13, 2023 WildFly 30 September 28, 2023 October 12, 2023 WildFly 31 December 14, 2023 January 11, 2024 WildFly 32 March 28, 2024 April 11, 2024 WildFly 33 June 27, 2024 July 11, 2024 WildFly 34 September 26, 2024 October 10, 2024 WildFly 35 December 12, 2024 January 9, 2024 WildFly is only roughly time boxed. We are certainly willing to delay a release to ensure adequate quality, and we would consider delaying a couple of weeks to include features of large enough significance. For example if a new MicroProfile release comes out in June and we need a bit more time to incorporate it in WildFly 29 we would consider delaying the WildFly 29 release somewhat. If we delayed a release we would not intend to then delay the next release; the next release would just have a shorter development window. The general idea though is to not block releasing waiting for features, as a quarterly cycle means a feature that misses a release can appear soon enough. MICRO RELEASES When we release each major we also create a new branch specific to that major. That branch is used to produce one micro (primarily bug fix) release for the major, with the micro usually released about a month after the major. This too is roughly time-boxed. Work on the WildFly 28.0.1 release, coordinated by Farah Juma, is currently in progress. The number of changes in the micro is typically small compared to what’s gone into main in the same period, as we want to be particularly conservative about introducing bugs or behavior changes in the micro. We’ve been consistently producing these micros since WildFly 17.0.1, and had done a few prior to that as well. QUESTIONS? If you have questions or want to provide feedback, I encourage you to post on the , on the or in . I will be starting a thread on the forum about this topic. Best regards, Brian</content><dc:creator>Brian Stansberry</dc:creator></entry><entry><title>How the new RHEL 9.2 improves the developer experience</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/10/how-new-rhel-92-improves-developer-experience" /><author><name>Nikhil Mungale, Alex Krikos</name></author><id>e5791719-da3c-4f3e-b3e1-d041098238cf</id><updated>2023-05-10T09:15:27Z</updated><published>2023-05-10T09:15:27Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) 9.2 is now Generally Available (GA). This release provides a flexible and stable foundation to support hybrid cloud innovations. Build, deploy, and manage applications and critical workloads faster and more efficiently with a consistent experience across physical, virtual, private, public cloud, and edge deployments. You can &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;download RHEL 9.2&lt;/a&gt; at no cost as part of the Red Hat Developer program subscription.&lt;/p&gt; &lt;p&gt;In this article, you will learn what's new in RHEL 9.2 and how it improves the developer experience.&lt;/p&gt; &lt;h2&gt;Latest language runtimes and tools&lt;/h2&gt; &lt;p&gt;Many of the popular languages in web development and enterprise applications have been upgraded in Red Hat Enterprise Linux 9.2 such as:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Python 3.11&lt;/strong&gt; is the latest version of the &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; programming language. This version includes the critical feature of Specializing Adaptive Interpreters. This feature optimizes operations, enabling code to run faster. Speed improvements from previous Python versions range from 10% to 50%, enhancing overall system performance. &lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Nginx 1.22&lt;/strong&gt; is the latest version of the Nginx lightweight web server, which brings new capabilities, such as OpenSSL 3.0 compatibility, hardening against request smuggling, and cross-protocol attacks. Red Hat Enterprise Linux 9.2 also supports Application-Layer Protocol Negotiation (ALPN) as part of the Nginx 1.22 update. This helps applications communicate with their peers securely by ensuring the client and server are using the most secure protocol available.&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;PostgreSQL 15&lt;/strong&gt; is the latest version of a popular open source database in Red Hat Enterprise Linux 9.2. PostgreSQL 15 has a more secure permission model in which create permission is revoked for all users except the database owner. This new permission model constrains ordinary users to user-private schemas. PostgreSQL 15 yields a fourfold improvement in row sorting speed for on-disk and in-memory sorts. This makes a compelling case for the PostgreSQL 15 upgrade, as this is a significant performance improvement over past versions.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;The latest versions of toolsets and compilers&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 offers an updated version of &lt;a href="https://developers.redhat.com/topics/rust"&gt;Rust&lt;/a&gt; 1.66, &lt;a href="https://developers.redhat.com/topics/go"&gt;Go&lt;/a&gt; 1.19, and LLVM 15, enabling developers to modernize their applications with the latest &lt;a href="https://developers.redhat.com/products/gcc-clang-llvm-go-rust/overview"&gt;toolsets and compilers&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Rust 1.66&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 comes with a stable version of Rust 1.66 with stabilized APIs and functions. Key updates in Rust 1.66 include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Newly stabilized &lt;black_box&gt; function. This helps disable optimizations when running the benchmarking of code or examining machine code that the compiler generates.&lt;/li&gt; &lt;li aria-level="1"&gt;Optimized APIs such as &lt;strong&gt;proc_macro::Span::source_text&lt;/strong&gt; and &lt;strong&gt;Option::unzip&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Go 1.19&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 comes with an updated version of Go 1.19. Most of the changes in Go 1.19 are related to the toolchain, runtime, and libraries. Notable changes include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The Go memory model has been revised to align with the memory model used by &lt;a href="https://developers.redhat.com/topics/c"&gt;C&lt;/a&gt;, C++, &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/javascript"&gt;JavaScript&lt;/a&gt;, Rust and Swift.&lt;/li&gt; &lt;li aria-level="1"&gt;Go 1.19 adds support for the Loongson 64-bit architecture, LoongArch, on &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;The Go runtime now supports a soft memory limit. This memory limit includes heap memory and all other memory managed by the runtime, excluding external memory sources. This allows Go application to maximize its available memory allocation and improve resource efficiency.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;LLVM 15&lt;/h3&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 comes with the LLVM 15 toolset with notable changes to the x86, ARM, and PowerPC back end. Notable changes include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;LLVM 15 now uses open pointers. Pointer types, including &lt;strong&gt;i8*&lt;/strong&gt;,&lt;strong&gt; i32*&lt;/strong&gt;,&lt;strong&gt; and void()**&lt;/strong&gt; are now represented as single ptr types.&lt;/li&gt; &lt;li aria-level="1"&gt;C APIs are updated with these additional functions: &lt;strong&gt;LLVMGetCastOpcode, LLVMGetAggregateElement, LLVMDeleteInstruction&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For more details, refer to the &lt;a href="https://releases.llvm.org/15.0.0/docs/ReleaseNotes.html#introduction"&gt;LLVM v15 release notes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Red Hat Enterprise Linux for Microsoft SQL Server&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux provides a secure, highly available, cost-effective, and consistent performance environment for Microsoft SQL-based applications that run on bare metal, virtual machines, &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, and the hybrid cloud.&lt;/p&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 comes with new system roles for MS SQL-based applications. Notable changes include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;MS SQL system roles allow authenticating directly with Microsoft Active Directory while configuring SQL server automatically.&lt;/li&gt; &lt;li aria-level="1"&gt;MS SQL system roles support automated installation and configuration of Microsoft SQL Server 2022.&lt;/li&gt; &lt;li aria-level="1"&gt;MS SQL system roles include an &lt;strong&gt;Always On&lt;/strong&gt; availability group support for asynchronous-commit mode and read-scale replica configurations.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Red Hat Enterprise Linux web console&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 brings new features to the web console making common configuration and management tasks easier by using an intuitive browser interface. Key changes in the Red Hat Enterprise Linux 9.2 web console include the following:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Enable network-bound disk encryption (NBDE) on the root file system using the web console.&lt;/li&gt; &lt;li aria-level="1"&gt;Disable root account logins on new Red Hat Enterprise Linux 9.2+ installations by default.&lt;/li&gt; &lt;li aria-level="1"&gt;Availability of frequently used combinations of policies and sub-policies of system-wide crypto policies. This enables adherence to industry-specific and site-specific standards.&lt;/li&gt; &lt;li aria-level="1"&gt;Showcase top consumers of disk I/O and network I/O which help administrators to identify which applications and workloads consume most of the resources.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Learn how to &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_systems_using_the_rhel_9_web_console/index"&gt;manage systems using the RHEL web console&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Security and compliance&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 simplifies how customers manage &lt;a href="https://developers.redhat.com/topics/security/"&gt;security&lt;/a&gt; and compliance while deploying new systems or managing existing infrastructure. Enhancements to security and compliance in RHEL 9.2 include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Availability of realmd iir system automates direct integration of RHEL systems with Microsoft Active Directory.&lt;/li&gt; &lt;li aria-level="1"&gt;Automated checking and hardening of systems by providing SCAP profile in accordance with the CIS benchmark for RHEL 9.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;RHEL system roles&lt;/h2&gt; &lt;p&gt;Red Hat Enterprise Linux 9.2 includes the following new features for system roles:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;New podman RHEL system role to automate the deployment of containers to save time and improve the consistency of deployment.&lt;/li&gt; &lt;li aria-level="1"&gt;New journald RHEL system role to automate persistence configuration of systemd journal on RHEL.&lt;/li&gt; &lt;li aria-level="1"&gt;New ad_integration system role to automate the integration of RHEL systems with Microsoft Active Directory.&lt;/li&gt; &lt;li aria-level="1"&gt;New rhc (remote host configuration) system to register RHEL systems automatically to Red Hat, including &lt;a href="https://www.redhat.com/en/technologies/management/insights"&gt;Red Hat Insights&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Enhancements to ha_cluster, logging, Microsoft SQL Server, and cockpit RHEL system roles to call certificate systems to generate corresponding certificates.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Learn more about &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/administration_and_configuration_tasks_using_system_roles_in_rhel/index"&gt;administration and configuration tasks using RHEL system roles.&lt;/a&gt;&lt;/p&gt; &lt;h2&gt;Downloads and more information&lt;/h2&gt; &lt;ul&gt;&lt;li&gt;Visit the RHEL product page to learn more about what's new in Red Hat Enterprise Linux 9.2 and &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;download RHEL 9.2 for free.&lt;/a&gt;  &lt;/li&gt; &lt;li&gt;Read the &lt;a href="https://www.redhat.com/en/about/press-releases/red-hat-delivers-latest-releases-red-hat-enterprise-linux"&gt;press release for the RHEL 9.2 announcement&lt;/a&gt;. &lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/10/how-new-rhel-92-improves-developer-experience" title="How the new RHEL 9.2 improves the developer experience"&gt;How the new RHEL 9.2 improves the developer experience&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nikhil Mungale, Alex Krikos</dc:creator><dc:date>2023-05-10T09:15:27Z</dc:date></entry><entry><title>What’s new in Red Hat’s migration toolkit for applications 6.1</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/10/whats-new-red-hats-migration-toolkit-applications-61" /><author><name>Yashwanth Maheshwaram</name></author><id>5b541f63-3596-45fe-9f88-70988e87dca1</id><updated>2023-05-10T07:00:00Z</updated><published>2023-05-10T07:00:00Z</published><summary type="html">&lt;p&gt;Red Hat's &lt;a href="https://developers.redhat.com/products/mta/overview"&gt;migration toolkit for applications&lt;/a&gt; helps development teams modernize and migrate their applications to &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, offering tools and best practices to accelerate the journey to &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. Developers can get access to the migration toolkit for applications with an &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;OpenShift&lt;/a&gt; subscription.&lt;/p&gt; &lt;h2&gt;User experience improvements&lt;/h2&gt; &lt;p&gt;Building on the success of the migration toolkit for applications 6.0, the 6.1 release adds more new features to make the user experience better. We've highlighted a few of these enhancements below.&lt;/p&gt; &lt;h3&gt;Custom migration targets&lt;/h3&gt; &lt;p&gt;Architects can now maintain custom rules in a repository, associate them to custom migration targets and make sure third-party developers use them in a straightforward way. This leads to faster and simpler analysis configuration when dealing with a series of well known custom technologies that are widespread across the whole application portfolio in an organization.&lt;/p&gt; &lt;h3&gt;Automated resource tagging&lt;/h3&gt; &lt;p&gt;The migration toolkit for applications now leverages the technology stack information that the analysis module is able to collect during an analysis and translates that into tags that get automatically added to an application in the inventory.&lt;/p&gt; &lt;h3&gt;Download of HTML and CSV analysis reports&lt;/h3&gt; &lt;p&gt;Application analysis now produces downloadable HTML and CSV reports. This option is disabled by default; you can be enable it in the new &lt;strong&gt;General&lt;/strong&gt; menu from the &lt;strong&gt;Administration&lt;/strong&gt; perspective.&lt;/p&gt; &lt;h3&gt;Allow architects to review an application without an assessment&lt;/h3&gt; &lt;p&gt;The requirement for running an assessment before being able to review an application has been removed. This option is disabled by default and can be enabled in the new &lt;strong&gt;General&lt;/strong&gt; menu from the &lt;strong&gt;Administration&lt;/strong&gt; perspective.&lt;/p&gt; &lt;h3&gt;Changes in naming&lt;/h3&gt; &lt;p&gt;Some entities and menu entries have been renamed for clarity:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;strong&gt;Administrator&lt;/strong&gt; and &lt;strong&gt;Developer&lt;/strong&gt; perspectives have been renamed to &lt;strong&gt;Administration&lt;/strong&gt; and &lt;strong&gt;Migration&lt;/strong&gt;, respectively.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Tag Types&lt;/strong&gt; are now named &lt;strong&gt;Tag Categories&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Disconnected installs&lt;/h3&gt; &lt;p&gt;After reviewing the deployment topology for migration toolkit for applications, this release now has full support for disconnected installs in air gapped OpenShift environments using the standard mechanism.&lt;/p&gt; &lt;h2&gt;Get started with the migration toolkit for applications&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/mta/getting-started"&gt;Follow these steps to get started with the migration toolkit for applications&lt;/a&gt; and begin with your &lt;a href="https://developers.redhat.com/topics/modernize-cloud-java"&gt;modernization&lt;/a&gt; journey today.&lt;/p&gt; &lt;p&gt;For a quick demo, watch the video of &lt;a href="https://www.youtube.com/watch?v=u9N-T-uD_KU"&gt;Red Hat Migration Toolkit for Applications 6.0&lt;/a&gt;.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/10/whats-new-red-hats-migration-toolkit-applications-61" title="What’s new in Red Hat’s migration toolkit for applications 6.1"&gt;What’s new in Red Hat’s migration toolkit for applications 6.1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yashwanth Maheshwaram</dc:creator><dc:date>2023-05-10T07:00:00Z</dc:date></entry><entry><title type="html">Quarkus 3.0.3.Final released - Maintenance release</title><link rel="alternate" href="https://quarkus.io/blog/quarkus-3-0-3-final-released/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-3-0-3-final-released/</id><updated>2023-05-10T00:00:00Z</updated><content type="html">We released Quarkus 3.0.3.Final, the second maintenance release of our 3.0 release train (as our first public release for 3.0 was 3.0.1.Final). As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 3.0. If you are not already using 3.0, please refer...</content><dc:creator>Guillaume Smet</dc:creator></entry><entry><title>Build and manage Red Hat Device Edge images with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/09/build-and-manage-red-hat-device-edge-images-ansible" /><author><name>Ricardo Noriega De Soto, James Harmison</name></author><id>a25423b2-f6d4-4f7f-860d-083ea58a8c53</id><updated>2023-05-09T07:00:00Z</updated><published>2023-05-09T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://www.redhat.com/en/technologies/device-edge"&gt;Red Hat Device Edge&lt;/a&gt; is a new solution that delivers a lightweight enterprise-ready &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; distribution called MicroShift combined with an &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge&lt;/a&gt;-optimized operating system built from &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL).&lt;/p&gt; &lt;p&gt;This article will guide you through the process of building your own customized Red Hat Device Edge images, from setting up the necessary building infrastructure to deploying the image on a device or virtual machine.&lt;/p&gt; &lt;p&gt;This tutorial will also show you a way to manage edge devices using supported tooling such as &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; in a pull-based model. This approach simplifies management and more accurately emulates real-life edge device connectivity.&lt;/p&gt; &lt;h2&gt;Building Red Hat Device Edge images&lt;/h2&gt; &lt;p&gt;Before you start to learn the process of creating and customizing Red Hat Device Edge images, we have pre-built one for you. Jump ahead to the section &lt;strong&gt;Test a pre-built Red Hat Device Edge image&lt;/strong&gt; so you can download the image and start testing. If you are interested in the process, keep reading.&lt;/p&gt; &lt;p&gt;The diagram in Figure 1 shows the architecture of what we are going to build in this tutorial.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhde_aap_blog_diagrams_-_imagebuilder.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhde_aap_blog_diagrams_-_imagebuilder.png?itok=BMOih1hB" width="600" height="339" alt="Image Builder architecture diagram" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: The required components to build and manage a Red Hat Device Edge image. You can build the image using image builder from a RHEL machine or use the pre-built image provided in this article. Once this image is flashed into your edge device, it is ready to be managed by Ansible and can be updated via an OSTree repository.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;First of all, we will need to set up our building machine. This is basically a Red Hat Enterprise Linux system where we are going to install a tool called &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/composing_a_customized_rhel_system_image/composer-description_composing-a-customized-rhel-system-image"&gt;image builder&lt;/a&gt;. Image builder allows you to build customized RHEL system images prepared for deployment on cloud platforms or on bare metal machines. Image builder automatically handles the setup details for each output type and therefore is easier to use and faster than other manual methods of image creation.&lt;/p&gt; &lt;h3&gt;Install image build tools&lt;/h3&gt; &lt;p&gt;Assuming there is already a RHEL 8 or RHEL 9 system with an active subscription you can work on (we recommend a disk size of 85 G to allow room for multiple images), let’s go ahead and install a set of packages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo dnf update -y sudo dnf install -y git osbuild-composer composer-cli cockpit-composer bash-completion lorax sudo systemctl enable --now osbuild-composer.socket &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This installs the required tools to build our customized images. Image builder works with a construct called &lt;strong&gt;sources&lt;/strong&gt;, which basically are RPM repositories where the tool is going to find extra packages not provided by the standard sources. MicroShift is shipped as part of the &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; repository, and it needs as a dependency, the Open vSwitch package shipped as part of the Fast Datapath repository.&lt;/p&gt; &lt;h3&gt;Add image builder sources&lt;/h3&gt; &lt;p&gt;In the next step, we will add these two repositories as image builder sources:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ARCH=$(uname -i) cat &lt;&lt; EOFRHOCP &gt; rhocp-source.toml check_gpg = true check_ssl = true id = "rhocp-4.12" name = "rhocp-4.12" rhsm = true system = false type = "yum-baseurl" url = "https://cdn.redhat.com/content/dist/layered/rhel8/$ARCH/rhocp/4.12/os" EOFRHOCP cat &lt;&lt; EOFFDP &gt; fdp-source.toml check_gpg = true check_ssl = true id = "fast-datapath" name = "fast-datapath" rhsm = true system = false type = "yum-baseurl" url = "https://cdn.redhat.com/content/dist/layered/rhel8/$ARCH/fast-datapath/os" EOFFDP composer-cli sources add rhocp-source.toml composer-cli sources add fdp-source.toml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After adding the sources, now you list them with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli sources list&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And see the following four sources:&lt;/p&gt; &lt;pre&gt; appstream baseos fast-datapath rhocp-4.12 &lt;/pre&gt; &lt;h3&gt;Create a blueprint for the Device Edge image&lt;/h3&gt; &lt;p&gt;Image builder has the concept of blueprint, a definition of the image to be built, where you can specify a set of packages, versions, and several customization options. &lt;a href="https://www.osbuild.org/guides/blueprint-reference/blueprint-reference.html"&gt;Refer to the upstream documentation.&lt;/a&gt; Now, we will create the blueprint for our Red Hat Device Edge image and push it to the image-builder service.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt; EOF &gt; microshift-container.toml name = "microshift-container" description = "" version = "0.0.1" distro = "rhel-8" modules = [] groups = [] # MicroShift and dependencies [[packages]] name = "microshift" version = "*" [[packages]] name = "openshift-clients" version = "*" [customizations] hostname = "edge" [customizations.services] enabled = ["microshift"] EOF composer-cli blueprints push microshift-container.toml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;depsolve&lt;/code&gt; sub-command will check if all packages are reachable from the build host and dependencies will be fulfilled, ensuring a better result during the build process. It will show a list of all packages included in the resulting image.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli blueprints depsolve microshift-container&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Create a rpm-ostree based RHEL image&lt;/h3&gt; &lt;p&gt;Red Hat Device Edge makes use of the benefits of certain technologies that are suitable for &lt;a href="https://developers.redhat.com/topics/edge-computing"&gt;edge computing&lt;/a&gt; scenarios. &lt;a href="https://coreos.github.io/rpm-ostree/"&gt;rpm-ostree&lt;/a&gt; is a technology that allows fully managed and reprovisionable operating system images with transactional upgrades and rollbacks. &lt;/p&gt; &lt;p&gt;Image builder provides a set of image types (such as qcow2, gce, ami, etc.) that includes rpm-ostree based images types, like edge-container, edge-commit, etc. If you want to find out more about these image types, read the &lt;a href="https://www.osbuild.org/guides/user-guide/building-an-image-from-cli.html"&gt;documentation&lt;/a&gt;. &lt;/p&gt; &lt;p&gt;For this purpose, we are going to create an OSTree-based Red Hat Enterprise Linux 8.7 image with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli compose start-ostree microshift-container --ref rhel/8/$ARCH/edge edge-container&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the status of the build with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ID=$(composer-cli compose list | grep "RUNNING" | awk '{print $1}') watch composer-cli compose list &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once it’s finished you can cancel the &lt;code&gt;watch&lt;/code&gt; with Ctrl+C. Download the image with the following commands: &lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli compose image $ID sudo chown $(id -u):$(id -g) $ID-container.tar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file is ready to be loaded into &lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;Podman&lt;/a&gt; and run as a &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt;. The container will expose the OSTree commit for image builder to pull and embed it into the installer image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman load -i $ID-container.tar IMAGE_ID=$(podman images | awk '/&lt;none&gt;/{print $3}') podman tag $IMAGE_ID localhost/microshift-container podman run -d --name=edge-container -p 8080:8080 localhost/microshift-container &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following steps will create an empty blueprint that will basically pull the rpm-ostree commit from the container we have just created, and embed it into the installer image. The resulting device will not point at the exposed rpm-ostree, but the local copy for the sake of simplicity and demonstration. In following steps, we will show you how to configure the device to point at the endpoint where the rpm-ostree is going to look for updates.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOF &gt; microshift-installer.toml name = "microshift-installer" description = "MicroShift Installer blueprint" version = "0.0.1" EOF composer-cli blueprints push microshift-installer.toml composer-cli blueprints depsolve microshift-installer composer-cli compose start-ostree microshift-installer edge-installer --ref rhel/8/$ARCH/edge --url http://localhost:8080/repo/ ID=$(composer-cli compose list | grep "RUNNING" | awk '{print $1}') watch -n 5 composer-cli compose list&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the image is ready, the status will show as &lt;code&gt;FINISHED&lt;/code&gt;. Cancel this &lt;code&gt;watch&lt;/code&gt; with Ctrl+C as well and download the resulting image with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;composer-cli compose image $ID&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The installer ISO image has been created.&lt;/p&gt; &lt;h3&gt;Inject and configure the kickstart file&lt;/h3&gt; &lt;p&gt;In order to have a fully automated installation experience, we will inject a kickstart file that configures several aspects such as LVM partitioning and firewall rules. Read through the kickstart file to understand what it is doing and you will find a couple of sections you might want to uncomment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt; EOF &gt; kickstart-microshift.ks lang en_US.UTF-8 keyboard us timezone UTC text reboot # Configure network to use DHCP and activate on boot network --bootproto=dhcp --device=link --activate --onboot=on # Partition disk with a 1GB boot XFS partition and an LVM volume containing a 10GB+ system root # The remainder of the volume will be used by the CSI driver for storing data # # For example, a 20GB disk would be partitioned in the following way: # # NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT # sda 8:0 0 20G 0 disk # ├─sda1 8:1 0 200M 0 part /boot/efi # ├─sda1 8:1 0 800M 0 part /boot # └─sda2 8:2 0 19G 0 part # └─rhel-root 253:0 0 10G 0 lvm /sysroot # zerombr clearpart --all --initlabel part /boot/efi --fstype=efi --size=200 part /boot --fstype=xfs --asprimary --size=800 part pv.01 --grow volgroup rhel pv.01 logvol / --vgname=rhel --fstype=xfs --size=10240 --name=root # Configure ostree ostreesetup --nogpg --osname=rhel --remote=edge --url=file:///run/install/repo/ostree/repo --ref=rhel/8/$ARCH/edge %post --log=/var/log/anaconda/post-install.log --erroronfail # Uncomment the following line and replace the rpm-ostree server name variable # in case you want to expose updates remotely at a certain URL # echo -e 'url=http://REPLACE_OSTREE_SERVER_NAME/repo/' &gt;&gt; /etc/ostree/remotes.d/edge.conf # The pull secret is mandatory for MicroShift builds on top of OpenShift, but not OKD # The /etc/crio/crio.conf.d/microshift.conf references the /etc/crio/openshift-pull-secret file cat &gt; /etc/crio/openshift-pull-secret &lt;&lt; EOFPS REPLACE_OCP_PULL_SECRET_CONTENTS EOFPS chmod 600 /etc/crio/openshift-pull-secret # Create a default redhat user, allowing it to run sudo commands without password useradd -m -d /home/redhat -p $(openssl passwd -6 redhat | sed 's/\$/\\$/g') -G wheel redhat echo -e 'redhat\tALL=(ALL)\tNOPASSWD: ALL' &gt;&gt; /etc/sudoers # Uncomment the following lines if you want to inject your ssh public key # mkdir -m 700 /home/redhat/.ssh # cat &gt; /home/redhat/.ssh/authorized_keys &lt;&lt; EOFK # REPLACE_REDHAT_AUTHORIZED_KEYS_CONTENTS # EOFK # chmod 600 /home/redhat/.ssh/authorized_keys # Make sure redhat user directory contents ownership is correct chown -R redhat:redhat /home/redhat/ # Configure the firewall (rules reload is not necessary here) firewall-offline-cmd --zone=trusted --add-source=10.42.0.0/16 firewall-offline-cmd --zone=trusted --add-source=169.254.169.1 # Make the KUBECONFIG from MicroShift directly available for the root user echo -e 'export KUBECONFIG=/var/lib/microshift/resources/kubeadmin/kubeconfig' &gt;&gt; /root/.profile %end EOF&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This kickstart file performs a set of steps to configure the host during the installation in a way required by MicroShift.&lt;/p&gt; &lt;p&gt;The device will need to have the OpenShift pull secret from the user injected so container images can be downloaded from the Red Hat registry. Follow this &lt;a href="https://console.redhat.com/openshift/install/pull-secret"&gt;link&lt;/a&gt;, copy your pull secret, and paste it in the following variable:&lt;/p&gt; &lt;pre&gt; PULL_SECRET=’’ # Don’t forget the single quotes to keep all characters from the pull secret&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sed -i "s/REPLACE_OCP_PULL_SECRET_CONTENTS/$PULL_SECRET/g" kickstart-microshift.ks sudo mkksiso kickstart-microshift.ks $ID-installer.iso redhat-device-edge-installer-$ARCH.iso&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Deploy the image&lt;/h3&gt; &lt;p&gt;Now you are ready to deploy the ISO you have customized into your device (or virtual machine). It should be located in &lt;code&gt;redhat-device-edge-installer-x86_64.iso&lt;/code&gt; in your working directory on the RHEL machine you've been working on. Recover it if necessary and flash it to a USB drive using &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/sect-making-usb-media#sect-making-usb-media-linux"&gt;this documentation&lt;/a&gt; or your own preferred method. The installation process is completely automated, and whenever the process is finished, you can log into your device using &lt;code&gt;redhat:redhat&lt;/code&gt; credentials. &lt;/p&gt; &lt;p&gt;MicroShift should be up and running, and the OpenShift client is ready to use. You can copy the kubeconfig file into the default location of your user:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir ~/.kube sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig &gt; ~/.kube/config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Or log in as root (&lt;code&gt;sudo -i&lt;/code&gt;) and it will be ready for you.&lt;/p&gt; &lt;pre&gt; $ oc get pods -A  NAMESPACE NAME READY STATUS RESTARTS AGE openshift-dns dns-default-87gk8 2/2 Running 0 111s openshift-dns node-resolver-tdpdc 1/1 Running 0 111s openshift-ingress router-default-54765bcdf7-qbvz7 1/1 Running 0 106s openshift-ovn-kubernetes ovnkube-master-l6xgk 4/4 Running 0 111s openshift-ovn-kubernetes ovnkube-node-gdwhj 1/1 Running 0 111s openshift-service-ca service-ca-79dbd484cf-kr549 1/1 Running 0 106s openshift-storage topolvm-controller-5c9ccfcf45-jzkdd 4/4 Running 0 112s openshift-storage topolvm-node-j8zbj 4/4 Running 0 112s &lt;/pre&gt; &lt;h2&gt;Test a pre-built Red Hat Device Edge image&lt;/h2&gt; &lt;p&gt;If you want to test a Red Hat Device Image right away, we've provided &lt;a href="https://developers.redhat.com/content-gateway/file/v1/Red-Hat-device-edge-x86-installer.iso"&gt;a pre-built image for you to download here&lt;/a&gt;. This image contains all the necessary components, except an OpenShift pull secret that you must insert once the device is up and running. Log into your device using &lt;code&gt;redhat:redhat&lt;/code&gt; credentials. &lt;/p&gt; &lt;p&gt;Copy your pull secret to the following location:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;/etc/crio/openshift-pull-secret&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, enable and start MicroShift:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo systemctl enable --now microshift.service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your device is now ready to be managed!&lt;/p&gt; &lt;h2&gt;Managing Red Hat Device Edge&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; is an &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; platform beloved by administrators in the data center and cloud for its simplicity (to learn and to use), flexibility, and advanced capabilities. Ansible Automation Platform is capable of managing any endpoint with a remote management interface, and includes out-of-the-box support for anything that can be managed over SSH, WinRM, and many API-driven environments (such as cloud providers and indeed, the Kubernetes API).&lt;/p&gt; &lt;p&gt;Ansible Automation Platform is designed to support a push-based, agentless management model. This flexibility means that managing new devices is as simple as updating a centralized inventory file with connection and authentication information, which is great for IT administrators looking to manage a large fleet of devices. The agentless architecture also gives us the flexibility to manage devices with a remote management interface that lack the capacity to install an agent, such as the common "appliance" form-factor devices in enterprise IT environments.&lt;/p&gt; &lt;h3&gt;Managing workloads on resource-constrained edge devices&lt;/h3&gt; &lt;p&gt;Red Hat Device Edge, being designed for very small form factor devices, finds its niche in the field rather than the data center or cloud. The OSTree deployment brings some nice features for that data center environment, but the features clearly target the edge—and the minimal-footprint Kubernetes at the heart of the Red Hat build of MicroShift clearly targets constrained resource environments who sacrifice features for size, weight, and power (SWAP).&lt;/p&gt; &lt;p&gt;This reality means that in some cases, we won’t control the network environment our Red Hat Device Edge nodes work in. Sometimes they’ll be on a constrained network hosted by a mobile provider; sometimes they’ll be mobile and connecting to WiFi networks opportunistically; sometimes they'll simply have very intermittent network connectivity and a push-based model might imply that a device is unhealthy when it’s simply not reachable.&lt;/p&gt; &lt;h3&gt;A more flexible approach&lt;/h3&gt; &lt;p&gt;Ansible does have a viable solution for these kinds of devices called &lt;a href="https://docs.ansible.com/ansible/latest/cli/ansible-pull.html"&gt;ansible-pull&lt;/a&gt;, but ansible-pull is only designed for situations where we want a device to manage only itself—and it strictly controls how Ansible inventories may be applied to the node running the command. Because MicroShift’s Kubernetes API is not the same management interface as Red Hat Device Edge’s SSH-based remote access at the OS layer, we might find ourselves in a situation where an application needs host configuration alongside the application deployment. We want a system that works like ansible-pull (rather than the traditional push model of Ansible Automation Platform), but gives us more flexibility.&lt;/p&gt; &lt;p&gt;Luckily, there are supported components of Ansible Automation Platform that make this kind of management model possible. It should be noted that this deployment architecture is not currently explicitly supported by Red Hat in production, but problems with the supported pieces we’re using should be able to be reproduced in a fully supported environment. If you’re a Red Hat customer interested in solving for these constraints then you should work with your account team at Red Hat to express interest.&lt;/p&gt; &lt;h3&gt;Architecture diagram&lt;/h3&gt; &lt;p&gt;The diagram in Figure 2 shows all the components we are going to use to manage edge devices with Ansible Automation Platform.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rhde_aap_blog_diagrams_-_ansible.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/rhde_aap_blog_diagrams_-_ansible.png?itok=VIRfRf_8" width="600" height="254" alt="AAP for edge architecture diagram" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: How Ansible Automation Platform manages the Red Hat Device Edge life cycle. A Git repository represents the source of truth for application and operating system configurations in code. Ansible will pull that content and operate on to the device to get to the desired state. Ansible can run as a Kubernetes CronJob or as a systemd service.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure the example repository&lt;/h3&gt; &lt;p&gt;With all of that stage-setting out of the way, let’s get started. Because we’re driving management of our Red Hat Device Edge node in a &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt;-based pull model, we need a repository to declare our configurations in.&lt;/p&gt; &lt;p&gt;It’s simplest to start from a working example, so head to &lt;a href="https://github.com/redhat-na-ssa/microshift-ansible-pull"&gt;https://github.com/redhat-na-ssa/microshift-ansible-pull&lt;/a&gt; and click the &lt;strong&gt;Fork&lt;/strong&gt; button in the top right to fork the repository into your own account or organization; then click &lt;strong&gt;Create fork&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;When GitHub finishes making your copy, clone the fork down to the RHEL 8 instance you used as the image builder in the preceding section using whatever method you prefer (GitHub CLI, SSH, HTTPS, your own IDE) and change into the repository root. If you have another RHEL device with valid entitlements and network access to your Red Hat Device Edge node, then it’s safe to run from that as well, but you won’t be able to run these commands from your Device Edge node.&lt;/p&gt; &lt;p&gt;This monorepo is designed to help you manage a single application deployed to a single MicroShift instance right now, but it should be extensible enough that once you get the idea of the workflow contained within you can extend it to fit your edge management use case. Let’s get to using the components included out of the box. Ensure that you have enabled the repositories for supported Ansible Automation Platform components:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo subscription-manager repos --enable ansible-automation-platform-2.3-for-rhel-$(rpm -qi redhat-release | awk '/Version/{print $3}' | cut -d. -f1)-x86_64-rpms&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Install the packages we’ll need:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sudo dnf -y install ansible-navigator openshift-clients&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Recover your kubeconfig from the MicroShift node. First, identify the IP address of the node from its own console, your DHCP server’s status page, or some other means—and set that IP address to a variable for easy consumption in later steps.&lt;/p&gt; &lt;pre&gt; RHDE_NODE=192.168.1.213 # MAKE SURE YOU SET THIS TO THE ACTUAL IP &lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir -p ~/.kube ssh redhat@$RHDE_NODE sudo cat /var/lib/microshift/resources/kubeadmin/kubeconfig &gt; ~/.kube/config sed -i 's/127\.0\.0\.1/'"$RHDE_NODE/" ~/.kube/config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In order to be able to reach the MicroShift API, we’re going to temporarily open the firewall on the Red Hat Device Edge node to port 6443. You should understand, for production systems, which management method is best for your edge endpoints. You can enable SSH and enforce strict security, or you can drop packets to SSH and open the path to the API endpoint (which doesn’t have any authentication/users, simply the built-in administrator who is identified by an X509 certificate), or you can choose to disable both once we get the pull-based management bootstrapped.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ssh redhat@$RHDE_NODE sudo firewall-cmd --add-port=6443/tcp oc get nodes # to verify that you have connectivity to the endpoint &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ensure that your terminal is in the repository root that you cloned down:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-apache"&gt;ls &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;ansible.cfg  ansible-navigator.yml  app  deploy  inventory  LICENSE  playbooks  README.md&lt;/code&gt;&lt;/p&gt; &lt;p&gt;Edit the inventory to point to your Red Hat Device Edge node:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sed -i 's/10\.1\.1\.11/'"$RHDE_NODE/" inventory/hosts &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Ensure that you can locally pull the supported execution environment using the &lt;code&gt;PULL_SECRET&lt;/code&gt; you saved earlier by putting it in one of the paths that Podman would find it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir -p ~/.docker echo "$PULL_SECRET" &gt; ~/.docker/config.json &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Make sure that the &lt;a href="https://docs.ansible.com/ansible/latest/vault_guide/index.html"&gt;Ansible Vault&lt;/a&gt; password for your secrets is saved in the location the repository expects it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;vault_pass="definitely a secure password" # This is actually the password encrypting the vault in the repository you forked mkdir -p /tmp/secrets echo "$vault_pass" &gt; /tmp/secrets/.vault &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If you changed the password for the &lt;strong&gt;redhat&lt;/strong&gt; user on the Red Hat Device Edge node from what was included in the kickstart, you’ll need to edit that. You might also want to change the vault password for the symmetrically encrypted Ansible Vault in the repository. You can do those with the following (optional) steps:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-vault () { podman run --rm -it --entrypoint ansible-vault -v ./:/repo -v /tmp/secrets/.vault:/tmp/secrets/.vault -e ANSIBLE_VAULT_PASSWORD_FILE=/tmp/secrets/.vault --security-opt=label=disable --privileged --workdir /repo registry.redhat.io/ansible-automation-platform-23/ee-supported-rhel8:1.0.0 "${@}" ; } ansible-vault decrypt inventory/group_vars/node/vault.yml ${EDITOR:-nano} inventory/group_vars/node/vault.yml # and update the password vault_pass="a totally new password" echo "$vault_pass" &gt; /tmp/secrets/.vault ansible-vault encrypt inventory/group_vars/node/vault.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our last little bit of configuration will be to point management to our own fork of the repo, instead of the default. It’s simplest right now to stick with HTTPS repository access for the Red Hat Device Edge node, but you could manage SSH deploy keys as secrets and mount them in place to access a repository over SSH, including a private repository, if it makes sense. Edit the configuration that defines where to look for updates:&lt;/p&gt; &lt;pre&gt; MY_GITHUB_ORG=redhat-fan-42 # set this to your actual GitHub org or user who forked the repository&lt;/pre&gt; &lt;pre&gt; &lt;code&gt;sed -i "s/redhat-na-ssa/$MY_GITHUB_ORG/" deploy/cronjob.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Let’s push our changes to the repository up to GitHub:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git status -u # make sure the changed files include only your vault (if applicable), inventory hosts, and the CronJob git diff # make sure the edits are what you expect (vault re-encrypted, hosts pointing to your IP, and the CronJob pointing to your fork git add . git commit -m 'Update for my environment' git push &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To ensure that we can push the bootstrapping configuration to the Red Hat Device Edge node, run the following command to use an Ansible “&lt;a href="https://docs.ansible.com/ansible/latest/collections/ansible/builtin/ping_module.html"&gt;ping&lt;/a&gt;” (which is much more than a simple ICMP ping):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-navigator exec --eev /tmp/secrets:/tmp/secrets:Z -- ansible microshift_node -m ping&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; microshift_node | SUCCESS =&gt; { "ansible_facts": { "discovered_interpreter_python": "/usr/libexec/platform-python" }, "changed": false, "ping": "pong" } &lt;/pre&gt; &lt;p&gt;With that, we’re ready to kick off our Red Hat Device Edge configuration bootstrapping!&lt;/p&gt; &lt;p&gt;The Ansible Automation Platform configuration could be embedded into the ISO or by using a provisioning system of your choice as well, instead of doing these manual steps. &lt;/p&gt; &lt;h3&gt;Bootstrap the MicroShift configuration&lt;/h3&gt; &lt;p&gt;Load the Ansible Vault secret manually into the MicroShift API:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc create namespace microshift-config oc create secret generic --from-literal=.vault="$vault_pass" vault-key -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;And run the playbooks locally, in push mode, using Ansible Navigator:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;for playbook in playbooks/*.yml; do ansible-navigator run $playbook -l microshift,microshift_node --eev ~/.kube/config:/home/runner/.kube/config:Z --eev /tmp/secrets/.vault:/tmp/secrets/.vault:Z; done&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Explore the &lt;code&gt;microshift-config&lt;/code&gt; namespace to see what we’ve bootstrapped into there:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get cm -n microshift-config&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME DATA AGE kube-root-ca.crt 1 2m22s microshift-ansible-pull 1 67s microshift-config-env 5 67s openshift-service-ca.crt 1 2m22s&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get cronjob -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGE microshift-config 15 * * * * False 0 76s&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pvc -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE microshift-config-data Pending topolvm-provisioner 83s runner-home Pending topolvm-provisioner 83s&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pod -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; No resources found in microshift-config namespace&lt;/pre&gt; &lt;p&gt;Some ConfigMaps, a CronJob (that hasn’t run yet), and two PVCs that will bind when the CronJob does finally run. Feel free to explore those ConfigMaps to better understand them, or dig into the README in the repository you cloned for a detailed explanation of what each of the pieces of this repository does.&lt;/p&gt; &lt;p&gt;Let’s look at the playbooks, now that everything’s set up:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat playbooks/cluster.yml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; --- - hosts: cluster tasks: - name: Create the namespace redhat.openshift.k8s: state: present api_version: v1 kind: Namespace name: hello - name: Deploy the application redhat.openshift.k8s: state: present namespace: hello src: '{{ playbook_dir }}/../app/{{ item }}' loop: - 00-deployment.yml - 05-svc.yml - 10-route.yml - name: Update the CronJob for the next run redhat.openshift.k8s: state: present src: '{{ playbook_dir }}/../deploy/cronjob.yml'&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat playbooks/node.yml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; --- - hosts: node become: true tasks: - name: Open firewall ports for the OpenShift Router ansible.posix.firewalld: state: enabled service: '{{ item }}' permanent: true immediate: true loop: - http - https&lt;/pre&gt; &lt;p&gt;These are pretty straightforward and even if you’ve never written a line of Ansible in your life, they should make sense pretty quickly. We’re going to target our MicroShift cluster with a playbook that configures a namespace and deploys an application from some manifests, and update our CronJob definition. Then we’re going to configure the firewall on the node to expose HTTP and HTTPS ports for the OpenShift Router to expose our web-based application.&lt;/p&gt; &lt;p&gt;Because we used ansible-navigator in a more traditional push management model, our changes are reflected right away. We have a CronJob that reaches out to Git for updates before applying the playbooks, so it can be reconciled without any external connectivity to its SSH or Kubernetes API ports.&lt;/p&gt; &lt;p&gt;It is important to highlight that the Ansible Automation Platform execution environment can be executed by a systemd service and a timer using Podman rather than a CronJob, if that's appropriate for your use case.&lt;/p&gt; &lt;p&gt;Let’s look at the route that was created. From your machine with the repository cloned (and the kubeconfig set up), run the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat app/10-route.yml &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; --- apiVersion: route.openshift.io/v1 kind: Route metadata: name: hello-world spec: port: targetPort: 8000 to: kind: Service name: hello-world weight: 100 wildcardPolicy: None tls: insecureEdgeTerminationPolicy: Redirect termination: edge &lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get route -n hello hello-world &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME HOST ADMITTED SERVICE TLS hello-world hello-world-hello.apps.example.com True hello-world &lt;/pre&gt; &lt;p&gt;Our MicroShift default configuration set the cluster’s router to be at &lt;code&gt;apps.example.com&lt;/code&gt;, so our route has a fake DNS name. Let’s add that to &lt;code&gt;/etc/hosts&lt;/code&gt; so we don’t have to update the cluster base domain or set up DNS on the network:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;HELLO_ROUTE=$(oc get route -n hello hello-world -ojsonpath='{.status.ingress[0].host}') echo "$RHDE_NODE $HELLO_ROUTE" | sudo tee -a /etc/hosts 10.1.1.11 hello-world-hello.apps.example.com &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then let’s curl that Route, accepting the default self-signed TLS certificate:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;curl -k https://hello-world-hello.apps.example.com&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; Hello, world, from hello-world-b8747f4d7-qc7v8!&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; oc get pods -n hello&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME READY STATUS RESTARTS AGE hello-world-b8747f4d7-j4bnd 1/1 Running 0 11h hello-world-b8747f4d7-qc7v8 1/1 Running 0 11h hello-world-b8747f4d7-qrpx6 1/1 Running 0 11h &lt;/pre&gt; &lt;p&gt;In my example here, it was the second replica that ended up answering the request. Let’s try some &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; management, changing something pretty straightforward about our application. Set the number of replicas to &lt;code&gt;1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;sed -i 's/replicas: 3/replicas: 1/' app/00-deployment.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Commit the changes and push to your remote repository without running ansible-navigator imperatively:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;git status -u git diff git add . git commit -m 'Changed to single replica' git push&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;br /&gt; The output of the previous commands should look like this:&lt;/p&gt; &lt;pre&gt; $ git status -u On branch main Your branch is up to date with 'origin/main'. Changes not staged for commit: (use "git add ..." to update what will be committed) (use "git restore ..." to discard changes in working directory) modified: app/00-deployment.yml no changes added to commit (use "git add" and/or "git commit -a") $ git diff diff --git a/app/00-deployment.yml b/app/00-deployment.yml index 2564981..3568124 100644 --- a/app/00-deployment.yml +++ b/app/00-deployment.yml @@ -6,7 +6,7 @@ metadata: labels: app: hello-world spec: - replicas: 3 + replicas: 1 selector: matchLabels: app: hello-world $ git add . $ git commit -m 'Changed to single replica' [main 4e96db7] Changed to single replica 1 file changed, 1 insertion(+), 1 deletion(-) $ git push Enumerating objects: 7, done. Counting objects: 100% (7/7), done. Delta compression using up to 32 threads Compressing objects: 100% (4/4), done. Writing objects: 100% (4/4), 810 bytes | 810.00 KiB/s, done. Total 4 (delta 2), reused 0 (delta 0), pack-reused 0 remote: Resolving deltas: 100% (2/2), completed with 2 local objects. To github.com:solacelost/microshift-ansible-pull.git 9c271fd..4e96db7 main -&gt; main&lt;/pre&gt; &lt;p&gt;Trigger a manual CronJob run, without waiting for the timer to fire off, with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;MANUAL_TIME=$(date +%s) oc create job --from=cronjob/microshift-config -n microshift-config microshift-config-manual-$MANUAL_TIME oc get pods -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At the time of this writing, you might be required to disable CSI Storage Capacity tracking to have the pod successfully create and bind the volumes. (Remember, MicroShift is still in Developer Preview!) To do that easily, you can run the following if your pod fails to schedule:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc patch csidriver topolvm.io -p '{"spec":{"storageCapacity":false}}' oc get pods -n microshift-config &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once your manual Job invocation has finished, you can check the logs to see what’s happened:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc logs -n microshift-config job/microshift-config-manual-$MANUAL_TIME &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; [...] PLAY RECAP ********************************************************************* microshift : ok=4 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 [...] PLAY RECAP ********************************************************************* microshift_node : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/pre&gt; &lt;p&gt;Note that on the cluster playbook, one thing changed. Nothing changed on the node. Let’s double-check our application now:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pods -n hello &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; NAME READY STATUS RESTARTS AGE hello-world-b8747f4d7-qc7v8 1/1 Running 1 12h&lt;/pre&gt; &lt;pre&gt; &lt;code&gt;curl -k https://hello-world-hello.apps.example.com&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; Hello, world, from hello-world-b8747f4d7-qc7v8! &lt;/pre&gt; &lt;p&gt;And, just like that, we’ve enabled asynchronous, GitOps-based replication of our Red Hat Device Edge node, including the MicroShift API layer.&lt;/p&gt; &lt;p&gt;Now we’re ready to begin adding more nodes to our inventory and expanding the method we use to apply our CronJob in the playbook to support this flexibility. Let’s look at the existing inventory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat inventory/hosts &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; [cluster] microshift ansible_connection=local ansible_python_interpreter=python3 some_other_microshift ansible_connection=local ansible_python_interpreter=python3 [node] microshift_node ansible_host=10.1.1.11 ansible_connection=ssh ansible_user=redhat some_other_microshift_node ansible_host=86.75.30.9 ansible_connection=ssh ansible_user=redhat&lt;/pre&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;And seeing that we’re managing &lt;code&gt;some_other_microshift&lt;/code&gt; in this repository, despite not having it available to us, we should probably consider how we’d apply the same methodology to another edge cluster. Check out the certified collection documentation for the &lt;a href="https://console.redhat.com/ansible/automation-hub/repo/published/redhat/openshift/content/module/k8s/"&gt;redhat.openshift.k8s&lt;/a&gt; module, and consider what we might do with the &lt;a href="https://docs.ansible.com/ansible/latest/collections/ansible/builtin/file_lookup.html"&gt;ansible.builtin.file&lt;/a&gt; lookup plug-in combined with the &lt;a href="https://docs.ansible.com/ansible/latest/playbook_guide/playbooks_filters.html#searching-strings-with-regular-expressions"&gt;regex_replace&lt;/a&gt; filter, alongside the &lt;a href="https://docs.ansible.com/ansible/latest/reference_appendices/special_variables.html"&gt;inventory_hostname&lt;/a&gt; special variable, available as part of our bootstrapping. A declarative inventory of many edge nodes running the same, or similar, applications are just a few more commits away with this framework.&lt;/p&gt; &lt;h2&gt;The bottom line&lt;/h2&gt; &lt;p&gt;Ansible Automation Platform is a very capable framework, able to meet your needs for robust and flexible management of endpoints with or without Ansible Controller. The depths of what you can accomplish, declaratively, with Ansible Automation Platform are limited only by your imagination—and, of course, the capabilities and reliability of the collection content at your disposal. If you need more robust workload management across your edge fleet, you could parameterize the shell script a bit further to prefer a branch named after the system UUID, but fall back to main, if desired.&lt;/p&gt; &lt;p&gt;Combining the power of Ansible Automation Platform with the full Kubernetes API available in MicroShift means that we have a wide array of tooling at our disposal for log aggregation, metrics, extensions via Kubernetes Operators, and more—in a fully edge-ready architecture.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/09/build-and-manage-red-hat-device-edge-images-ansible" title="Build and manage Red Hat Device Edge images with Ansible"&gt;Build and manage Red Hat Device Edge images with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ricardo Noriega De Soto, James Harmison</dc:creator><dc:date>2023-05-09T07:00:00Z</dc:date></entry><entry><title type="html">MEF and Sofis use Quarkus as core component of a new innovative architecture</title><link rel="alternate" href="https://quarkus.io/blog/ministry-of-economy-finance-uruguay-adopts-quarkus/" /><author><name>Fabricio Gregorio</name></author><id>https://quarkus.io/blog/ministry-of-economy-finance-uruguay-adopts-quarkus/</id><updated>2023-05-09T00:00:00Z</updated><content type="html">About us The Ministry of Economy and Finance (MEF) of Uruguay is a government ministry. Website: https://www.gub.uy/ministerio-economia-finanzas Sofis Solutions is a company with more than 18 years of experience in the digital transformation of Latin American organizations and technological inclusion. Focuses mainly on projects for the development of digital government...</content><dc:creator>Fabricio Gregorio</dc:creator></entry><entry><title>How to create execution environments using ansible-builder</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/05/08/how-create-execution-environments-using-ansible-builder" /><author><name>Tathagata Paul</name></author><id>7453d38e-90f7-4d30-9fba-9db9e244053d</id><updated>2023-05-08T07:00:00Z</updated><published>2023-05-08T07:00:00Z</published><summary type="html">&lt;p&gt;The execution environment builder (aka Ansible Builder) is a part of &lt;a href="https://developers.redhat.com/products/ansible/"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. It is a command-line interface (CLI) tool for building and creating custom execution environments. The Ansible Builder project enables users to automate and accelerate the process of creating execution environments. This article will show you how to install and use the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.1/html/ansible_builder_guide/index"&gt;execution environment builder&lt;/a&gt; CLI tool.&lt;/p&gt; &lt;h2&gt;Installing the execution environment builder&lt;/h2&gt; &lt;p&gt;The execution environment builder makes it easier for Ansible Automation Platform content creators and administrators to build custom execution environments. They can use dependency information from various &lt;a href="http://ansible.com/products/content-collections"&gt;Ansible Content Collections&lt;/a&gt; and directly from the user.&lt;/p&gt; &lt;h3&gt;Step 1: Install the execution environment builder tool&lt;/h3&gt; &lt;p&gt;Install the execution environment builder tool from the Python Package Index (PyPI) by using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;pip install ansible-builder&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 2: Access the ansible-builder subcommands&lt;/h3&gt; &lt;p&gt;To access the subcommands of &lt;code&gt;ansible-builder&lt;/code&gt;, run &lt;code&gt;build&lt;/code&gt; and &lt;code&gt;create&lt;/code&gt; commands to get help output.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;build&lt;/code&gt; subcommand will build the execution environment using the definition file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build –help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It populates the build context and then uses Podman or Docker to create the execution environment image. The help output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;usage: ansible-builder build [-h] [-t TAG] [--container-runtime {podman,docker}] [--build-arg BUILD_ARGS] [-f FILENAME] [-c BUILD_CONTEXT] [--output-filename {Containerfile,Dockerfile}] [-v {0,1,2,3}] Creates a build context (including a Containerfile) from an execution environment spec. The build context will be populated from the execution environment spec. After that, the specified container runtime podman/docker will be invoked to build an image from that definition. After building the image, it can be used locally or published using the supplied tag. optional arguments: -h, --help show this help message and exit -t TAG, --tag TAG The name for the container image being built (default: ansible-execution-env:latest) --container-runtime {podman,docker} Specifies which container runtime to use (default: podman) --build-arg BUILD_ARGS Build-time variables to pass to any podman or docker calls. Internally ansible-builder makes use of ANSIBLE_GALAXY_CLI_COLLECTION_OPTS, EE_BASE_IMAGE, EE_BUILDER_IMAGE. -f FILENAME, --file FILENAME The definition of the execution environment (default: execution-environment.yml) -c BUILD_CONTEXT, --context BUILD_CONTEXT The directory to use for the build context (default: context) --output-filename {Containerfile,Dockerfile} Name of file to write image definition to (default depends on --container-runtime, Containerfile for podman and Dockerfile for docker) -v {0,1,2,3}, --verbosity {0,1,2,3} Increase the output verbosity, for up to three levels of verbosity (invoked via "--verbosity" or "-v" followed by an integer ranging in value from 0 to 3) (default: 2) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;create&lt;/code&gt; subcommand works similar to the &lt;code&gt;build&lt;/code&gt; command.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder create –help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;However, it will not build the execution environment image as you will see in the following output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;usage: ansible-builder build [-h] [-t TAG] [--container-runtime {podman,docker}] [--build-arg BUILD_ARGS] [-f FILENAME] [-c BUILD_CONTEXT] [--output-filename {Containerfile,Dockerfile}] [-v {0,1,2,3}] Creates a build context (including a Containerfile) from an execution environment spec. The build context will be populated from the execution environment spec. After that, the specified container runtime podman/docker will be invoked to build an image from that definition. After building the image, it can be used locally or published using the supplied tag. optional arguments: -h, --help show this help message and exit -t TAG, --tag TAG The name for the container image being built (default: ansible-execution-env:latest) --container-runtime {podman,docker} Specifies which container runtime to use (default: podman) --build-arg BUILD_ARGS Build-time variables to pass to any podman or docker calls. Internally ansible-builder makes use of ANSIBLE_GALAXY_CLI_COLLECTION_OPTS, EE_BASE_IMAGE, EE_BUILDER_IMAGE. -f FILENAME, --file FILENAME The definition of the execution environment (default: execution-environment.yml) -c BUILD_CONTEXT, --context BUILD_CONTEXT The directory to use for the build context (default: context) --output-filename {Containerfile,Dockerfile} Name of file to write image definition to (default depends on --container-runtime, Containerfile for podman and Dockerfile for docker) -v {0,1,2,3}, --verbosity {0,1,2,3} Increase the output verbosity, for up to three levels of verbosity (invoked via "--verbosity" or "-v" followed by an integer ranging in value from 0 to 3) (default: 2) &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3: Populate the ansible-builder spec&lt;/h3&gt; &lt;p&gt;Populate the &lt;code&gt;ansible-builder&lt;/code&gt; spec to build the custom execution environment by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;mkdir project_directory &amp;&amp; cd project_directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Populate the &lt;code&gt;execution-environment.yml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; execution-environment.yml --- version: 1 dependencies: galaxy: requirements.yml EOT &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a &lt;code&gt;requirements.yml&lt;/code&gt; file and populate the contents with the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.yml --- collections: - name: servicenow.itsm EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Through the spec and requirements file, we ensure that execution environment builder will download the &lt;strong&gt;servicenow.itsm collection&lt;/strong&gt; while building the execution environment. The default download location is &lt;strong&gt;galaxy.ansible.com&lt;/strong&gt;. You can also point to an automation hub or your own hub instance in the spec file.&lt;/p&gt; &lt;h3&gt;Step 4: Build the execution environment&lt;/h3&gt; &lt;p&gt;Build the execution environment using the previously created files. Run the following command to create a new custom execution environment called &lt;strong&gt;custom-ee:&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build -v3 -t custom-ee&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;-v3&lt;/code&gt; flag adds verbosity to the CLI run, and &lt;code&gt;-t custom-ee&lt;/code&gt; will tag your image with the name you provided.&lt;/p&gt; &lt;p&gt;The output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Ansible Builder is building your execution environment image, "custom-ee". File context/_build/requirements.yml will be created. Rewriting Containerfile to capture collection requirements Running command: podman build -f context/Containerfile -t custom-ee context [1/3] STEP 1/7: FROM registry.redhat.io/ansible-automation-platform-21/ee-minimal-rhel8:latest AS galaxy [1/3] STEP 2/7: ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= --&gt; 88d9ea223d0 [1/3] STEP 3/7: USER root --&gt; 549f29055c2 [1/3] STEP 4/7: ADD _build /build --&gt; 0d3e9515b12 [1/3] STEP 5/7: WORKDIR /build --&gt; 3b290acf78c [1/3] STEP 6/7: RUN ansible-galaxy role install -r requirements.yml --roles-path /usr/share/ansible/roles Skipping install, no requirements found --&gt; 8af36370e78 [1/3] STEP 7/7: RUN ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path /usr/share/ansible/collections Starting galaxy collection install process Process install dependency map … &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following commands to check the image list:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman images&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output appears as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;REPOSITORY TAG IMAGE ID CREATED SIZE localhost/custom-ee latest bfe6c40bad52 21 seconds ago 626 MB &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 5: Build a complex execution environment&lt;/h3&gt; &lt;p&gt;To build a complex execution environment, go back into the project directory with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd project_directory&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Edit the &lt;code&gt;execution-environment.yml&lt;/code&gt; file and add the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; execution-environment.yml --- version: 1 dependencies: galaxy: requirements.yml python: requirements.txt system: bindep.txt additional_build_steps: prepend: | RUN whoami RUN cat /etc/os-release append: - RUN echo This is a post-install command! - RUN ls -la /etc EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can see the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Python requirements were added through the &lt;strong&gt;requirements.txt&lt;/strong&gt; file, which will hold the pip dependencies.&lt;/li&gt; &lt;li&gt;We added a &lt;strong&gt;bindep.txt&lt;/strong&gt;, which will hold the rpm installs.&lt;/li&gt; &lt;li&gt;Additional build steps that will run before (prepend) and after (append) the build steps.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Now create a new file called &lt;code&gt;requirements.yml&lt;/code&gt; and append the following content:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.yml --- collections: - name: servicenow.itsm - name: ansible.utils EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We added a new collection called &lt;strong&gt;ansible.utils&lt;/strong&gt; alongside the &lt;strong&gt;servicenow.itsm&lt;/strong&gt; file.&lt;/p&gt; &lt;p&gt;Create a new file called &lt;code&gt;requirements.txt&lt;/code&gt; and then append the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; requirements.txt gcp-cli ncclient netaddr paramiko EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This contains the &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; requirements that need to be installed via pip.&lt;/p&gt; &lt;p&gt;Create a new file called &lt;code&gt;bindep.txt&lt;/code&gt; and then append the following:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat &lt;&lt;EOT &gt;&gt; bindep.txt findutils [compile platform:centos-8 platform:rhel-8] gcc [compile platform:centos-8 platform:rhel-8] make [compile platform:centos-8 platform:rhel-8] python38-devel [compile platform:centos-8 platform:rhel-8] python38-cffi [platform:centos-8 platform:rhel-8] python38-cryptography [platform:centos-8 platform:rhel-8] python38-pycparser [platform:centos-8 platform:rhel-8] EOT&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This file contains the rpm requirements needed to be installed using dnf.&lt;/p&gt; &lt;p&gt;Run the following build:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-builder build -v3 -t custom-ee&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The output is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Ansible Builder is building your execution environment image, "custom-ee". File context/_build/requirements.yml will be created. File context/_build/requirements.txt will be created. File context/_build/bindep.txt will be created. Rewriting Containerfile to capture collection requirements Running command: podman build -f context/Containerfile -t custom-ee context [1/3] STEP 1/7: FROM registry.redhat.io/ansible-automation-platform-21/ee-minimal-rhel8:latest AS galaxy [1/3] STEP 2/7: ARG ANSIBLE_GALAXY_CLI_COLLECTION_OPTS= --&gt; Using cache 88d9ea223d01bec0d53eb7efcf0e76b5f7da0285a411f2ce0116fe9641cbc3a0 --&gt; 88d9ea223d0 [1/3] STEP 3/7: USER root --&gt; Using cache 549f29055c2f1ba0ef3f7c5dfdc67a40302ff0330af927adb94fbcd7b0b1e7b4 --&gt; 549f29055c2 [1/3] STEP 4/7: ADD _build /build --&gt; 6b9ee91e773 [1/3] STEP 5/7: WORKDIR /build --&gt; 5518e019f2d [1/3] STEP 6/7: RUN ansible-galaxy role install -r requirements.yml --roles-path /usr/share/ansible/roles Skipping install, no requirements found --&gt; 60c1605d66c [1/3] STEP 7/7: RUN ansible-galaxy collection install $ANSIBLE_GALAXY_CLI_COLLECTION_OPTS -r requirements.yml --collections-path /usr/share/ansible/collections Starting galaxy collection install process Process install dependency map&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the context or &lt;code&gt;Containerfile&lt;/code&gt; to see all the steps you took to build the execution environment. You can transfer the context directory to a different server and replicate the image creation via &lt;code&gt;docker&lt;/code&gt; or &lt;code&gt;podman&lt;/code&gt; commands.&lt;/p&gt; &lt;h2&gt;Pushing the execution environment to a private automation hub&lt;/h2&gt; &lt;p&gt;Log in to the &lt;a href="https://www.ansible.com/blog/control-your-content-with-private-automation-hub"&gt;private automation hub&lt;/a&gt; by using the &lt;code&gt;podman&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman login &lt;automation hub url&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then tag the image before pushing it to the hub as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman tag localhost/custom-ee &lt;automation hub url&gt;/developers-bu-aap-builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Finally, push it to the private automation hub as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman push &lt;automation hub url&gt;/developers-bu-aap-builder&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can see the image pushed to the private automation hub in Figure 1:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/image_1.png?itok=AxLHdHot" width="1440" height="816" alt="The private automation hub page showing multiple pushed execution environment images." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The private automation hub page showing multiple pushed execution environment images.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Continue your automation journey with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started with Ansible Automation Platform&lt;/a&gt; by exploring &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;interactive labs&lt;/a&gt;. Check out Red Hat’s hands-on labs for all skill levels to learn more. The wide range of labs include &lt;a href="https://developers.redhat.com/learn/lessons/linux-commands?intcmp=7013a0000026UTXAA2"&gt;useful Linux commands&lt;/a&gt;, &lt;a href="https://developers.redhat.com/learn/installing-software-using-package-managers?intcmp=7013a0000026UTXAA2"&gt;Install software using package managers&lt;/a&gt;, and &lt;a href="https://developers.redhat.com/learn/lessons/deploying-containers-podman?intcmp=7013a0000026UTXAA2"&gt;Deploying containers using container tools [podman]&lt;/a&gt;. Try these labs to see your favorite products in action. Ansible Automation Platform is also available as a managed offering on&lt;a href="https://www.redhat.com/en/technologies/management/ansible/azure"&gt; Microsoft Azure&lt;/a&gt; and as a self-managed offering on &lt;a href="https://www.redhat.com/en/technologies/management/ansible/aws"&gt;AWS&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/05/08/how-create-execution-environments-using-ansible-builder" title="How to create execution environments using ansible-builder"&gt;How to create execution environments using ansible-builder&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Tathagata Paul</dc:creator><dc:date>2023-05-08T07:00:00Z</dc:date></entry></feed>
